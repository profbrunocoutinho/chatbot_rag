# ğŸ¦¥ Chatbot RAG com LLaMA (ExecuÃ§Ã£o Local)

Este projeto Ã© um chatbot orquestrador do tipo **RAG (Retrieval-Augmented Generation)** utilizando o modelo **LLaMA ou Mistral quantizado (.gguf)** para execuÃ§Ã£o local com `llama-cpp-python`.

O chatbot permite consultas semÃ¢nticas a documentos PDF indexados via FAISS, com interface web desenvolvida em **Streamlit**.

---

## âœ… Tecnologias Utilizadas

- Python 3.10+
- Streamlit
- LangChain
- FAISS (persistÃªncia vetorial)
- Hugging Face Embeddings (`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`)
- `llama-cpp-python` (modelo local quantizado)
- Modelos suportados: LLaMA, Mistral ou compatÃ­veis no formato `.gguf`

---

## âš™ï¸ PrÃ©-Requisitos

- Python 3.10 ou superior instalado
- Compilador C/C++ configurado (Visual Studio Build Tools no Windows)
- CMake instalado
- `llama-cpp-python` corretamente instalado (compilado)
- Arquivo `.gguf` do modelo LLaMA ou Mistral baixado
- Base FAISS previamente criada com os documentos PDF

---

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/chatbot-rag-llama.git
cd chatbot-rag-llama
```

### 2. Crie e ative um ambiente virtual

```bash
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux
```

### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Baixe um modelo `.gguf` compatÃ­vel

- Recomendo: [Mistral 7B Instruct GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
- Coloque o modelo baixado na pasta: `./modelos/`

---

## ğŸ—‚ï¸ Estrutura do Projeto

```text
chatbot-rag-llama/
â”œâ”€â”€ app.py                # Interface Streamlit
â”œâ”€â”€ processa_base.py      # Processa PDFs e cria a base FAISS
â”œâ”€â”€ faiss_index/          # Base vetorial persistida
â”œâ”€â”€ modelos/              # Pasta onde deve estar o arquivo .gguf
â”œâ”€â”€ requirements.txt      # DependÃªncias
â””â”€â”€ README.md
```

---

## ğŸ–¥ï¸ Como Executar

### 1. Processar a base de documentos (se ainda nÃ£o fez)

```bash
python processa_base.py
```

### 2. Rodar o chatbot

```bash
streamlit run app.py
```

### 3. Acesse no navegador

```text
http://localhost:8501
```

---

## âš™ï¸ ConfiguraÃ§Ãµes importantes no `app.py`

- Caminho do modelo:

```python
caminho_modelo = r"C:\Users\SeuUsuario\Documents\chatbot-rag-llama\modelos\mistral-7b-instruct-v0.2.Q4_K_M.gguf"
```

- ParÃ¢metros ajustÃ¡veis:

```python
temperature=0.7
max_tokens=512
top_p=0.95
n_ctx=2048
```

---

## ğŸ“š ReferÃªncias

- [LangChain](https://github.com/langchain-ai/langchain)
- [FAISS](https://github.com/facebookresearch/faiss)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [TheBloke Models - Hugging Face](https://huggingface.co/TheBloke)

---

## ğŸ›¡ï¸ LicenÃ§a

Este projeto utiliza modelos de cÃ³digo aberto sob licenÃ§as compatÃ­veis (Apache 2.0, MIT ou permissivas).\
O uso de modelos LLaMA da Meta requer aceitaÃ§Ã£o da licenÃ§a de uso nÃ£o comercial.

---

## âœ‰ï¸ Contato

Desenvolvido por Bruno â€“ Projeto educacional para uso no contexto do Ifes.

